<!doctype html>
<html lang="en">
  <head>
    <title>Caleb Flight Research Paper</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <style type="text/css" media="screen">
        @import url( ../shared/style.css );
    </style>
    <script src="../shared/scripts/dmceFunctions.js" type="text/javascript"></script>

  </head>
<body class="cursor" style="width:100%; height:100%">

<div class="book">
    <div class="page" id="0">
        <div class="subpage">
            <p style="text-align:center"><br><br><strong>Simulation to Application. The Use of Computer Simulations to Improve Real-World Application of Learning.</strong></p>
        </div>    
    </div>
    
    <div class="page" id="1">
        <div class="subpage">
            <p><strong>Abstract:</strong> Simulations have been used in training and education for years to aid students in gaining the skills needed to complete a task in a low risk environment. However, students can have trouble connecting the skills used in the simulated working environments to skills that are needed to be applied in the real-world environment, referred to as adaptive transfer. The simulations referred to in this study are simulated environments that mirror student skill application, not a simulation of an event that is meant to aid students in the development of concept knowledge around the demonstrated event. This study examines students' ability to transfer skills learned during a simulation activity to that of a real-world application setting. The study is situated within an introductory engineering computing course in which students are required to work within MyITLab to gain familiarity with using Microsoft Office Software, specifically Microsoft Excel. In this setting, students are expected to use high fidelity simulations, complete online course work based upon these simulations, and then complete a comprehensive exam to demonstrate skills learned with the real-world application.</p>
            <p> 
            <span style="display:inline !important; float:right" id="mark" class=""><img src="img/icn-point-annotator-navy.png" width="40" height="40" class="icnh" alt="icon"></span><mark>Guided by Kolb’s experiential learning theory</mark><sup>1</sup>, end of course surveys were implemented to investigate student self-efficacy, the adaptive transfer process, and students’ perceived ability to successfully use this software for real world productivity outside of the classroom environment.</p>

            <p>Survey questions focused upon the student experience when working with simulation software and how using the software enabled them to use the Microsoft Office Software effectively. Survey results were correlated with course grades from preparation simulation activities and the final application exam. By gaining a better understanding of how students transfer knowledge from the simulated activity environment to the application environment, while capturing individual learning preferences, instructors will be able to better aid students to more effectively transition skills between different types of environments and create a more holistic learning environment that convert theoretical understanding into practical application. The lessons learned from this study will be used to inform the implementation of improved practices when the course is delivered the following term.​
            </p>
            
            <h2>I. Introduction</h2>
            <p> “When used accordingly, simulation can enhance a student’s problem solving skills,”<sup>2</sup> (p. 1)
            <br><br>
            Within engineering education, there is a constant effort to prepare students to enter the professional world. With the adoption of the ABET EC2000 criteria and the a-k program outcomes, professional skills have moved to the forefront of the engineering curricula<sup>3</sup>.  This specific project began with a course design challenge around injecting professional skills into a unique course that teaches basic computing and business skills to students from three separate colleges at one university. Industry Advisory committee members across the curriculum suggest a real need for students to develop and maintain skills important to their specialized field of study, but can be integrated with specific business elements such as, word processing, professional writing, and budgetary management. Scachitti also highlighted this multidisciplinary challenge stating, whether students find employment in manufacturing, healthcare or service industries, they will all be faced with decision making and problem solving involving increasingly complex systems and rapidly changing technology.2 Therefore, this course became a uniquely positioned opportunity to bring together business, technology, and general professional skills training through hands-on opportunities. 
            </p>
        </div>    
    </div>

    <div class="page" id="2">
        <div class="subpage">
            <p>Since technical engineering courses often contain hands-on laboratory activities to emphasize course concepts<sup>4</sup>, it became apparently that this course should contain similar learning components for teaching professional skills mainly using simulations. This was supported by the adaptive nature of this course, which is continually redesigned to maintain its relevance in the area of technology. Thus, new technology offerings, such as simulations, are implemented every two years, while maintaining the historical elements of industry practices that do not waiver, such as the history of the Internet and Circuitry.</p>
            <p>The course under examination not only contains a lecture component, but a hands-on computer lab component, which include the simulations. The hands-on lab component allows students the opportunity to actively use the required software through a self-paced simulation. This allows students to learn the new technology programs at their own pace. The lecture component then focuses on bring the application of this skill into an application perspective where they can situate their new skills. The course culminates in an exam that contextualizes a problem and requires the use of the simulated skills.</p>
            <p>While simulations are not a new learning activity within engineering curriculum5, high-fidelity simulations for the teaching of business productivity skills (mainly the use of Microsoft Office software) appear to have limited exploration. Another unique element of this course, and the study surrounding it, is the volume of specific student feedback as part of the overall exam improvement. Students were asked through a discussion activity to relate their exam experience regarding how content in the exam related to previous practice offerings; thus, asking specifically about the marriage between simulation training and lecture content to that of exam content and structure. This was supplemented with a survey that explicitly targeted certain areas of the transfer between the skills used in the simulated environment and the skills used on the final exam replicating a real-world problem and application.</p>
            <p>The purpose of this study is to examine students’ perceived identification of concepts tested within an engineering course redesign capitalizing on Kolb’s Experiential Learning Theory<sup>1</sup> in order to examine the holistic nature of theoretical to conceptual application when using a high-fidelity simulated environment. </p>
            <h3>Research Questions</h3>
            <ol>
                <li>How does student interaction with a simulation transition the students to the application of skills in the real world?</li>
                <li>What skills can students identify and connect between the simulation and a real-world application?</li>
            </ol>
            <h2>II. Literature Review</h2>
            <h3>Simulations in the Classroom</h3>
            <p>“The pedagogical value of the hands-on experience that a laboratory provides is ubiquitously endorsed by educators,”<sup>6</sup> (p. 541). However, true hands-on activities may not exist or be available for all type of industries, such as within health care and core engineering sciences fields due to cost, access, or ethical obligations. For these fields where direct hands-on experience is not available, simulations can offer many benefits over learning environments that provide no support or training for the learning and practicing of real-world industry skills<sup>7</sup>.</p>
        </div>
    </div>

    <div class="page" id="3">
        <div class="subpage">
            <p> “Simulations promote active learning, especially at the stage of debates that arise because of the complexity, interconnectedness, and novelty of decision-making. Additionally, simulations develop critical and strategic thinking skills”<sup>5</sup> (p. 290). Therefore, simulations have become an expansive learning tool that bridges all industries in a flexible manner<sup>2</sup>.</p>
            <p>Despite the ongoing use of simulations in education and industry, the types and definition of a simulation vary across the literature. Curtin8 explored the area of multimodal which highlights optimal preparation and proper sequencing when using simulations as a learning activity. De Jong and van Joolingen9 (1998) indicated that the “the main task of the learner [is] to infer, through experimentation, characteristics of the model underlying the simulation” (p. 180), yet no additional description was given.  Unlike Kolloffel and de Jong<sup>10</sup> who examined simulations that allowed students to change variables within a circuitry environment, their description was summarized by the term “virtual lab.” For our study, we will use the characterization by Davidovitch5 “as tools enabling the acquisition of practical experience and acceptance of an immediate response of the learned system to the user’s decisions and actions” (p.  290). This definition brings clarity to the term simulation and fortifies Davidovitch’s statement that simulations are “recognized as an efficient and effective way of teaching and learning complex and dynamic systems for engineering education,”<sup>5</sup> (p. 290).</p>
            <p>There are many advantages when using simulations as the educator can create an environment of “practice without risk,”<sup>11</sup> (p. e10). Thus, supporting the notion of reducing “the gaps between the learning environment and the ‘real’ environment, and the availability of training in situations that are difficult to obtain and practice in the ‘real world’”<sup>5</sup> (p. 290). Simulations simply allow the end user to become active within their personal learning activity and in turn “support authentic inquiry practices that include formulating questions, hypothesis development, data collection, and theory revision,”<sup>12</sup> (p. 136).</p>
            <p>However, despite the rally for using simulations within a learning activity, “there is a danger that students disengage from connecting to the underlying process being simulated and instead transition into a computer game mode,”<sup>6</sup> (p. 547). By moving into a “gaming” mode, the transfer of the skills learned may be limited. However, “Simulations contain models that are designed to simulate systems, processes, or phenomena,”<sup>10</sup> (p. 278), allowing for the benefit of “standardization and repetition of content”<sup>7</sup> (p. 151). This provides invaluable experience for the adaptive learning process. Therefore, educators must continually investigate student learning construction and ensure the activity is meeting the intended learning objectives and goals when simulations area implemented within a learning activity<sup>6</sup>.
            <p>For the current investigation, the simulations require specific steps to be completed in order to receive credit; thus, requiring students to repeat specific steps in order reinforce conceptual knowledge and procedural skills. Students are additionally provided support in the form of audio, visual, and step-by-step instruction to overcome any deficiencies in reading comprehension or for the hearing and visually impaired. Therefore, the simulations under investigation do provide opportunities to initially explore the system using a guide. The margin of testing is limited in order to highlight best procedures needed to work within the system most effectively. </p>
        </div>
    </div>

    <div class="page" id="4">
        <div class="subpage">
            <h2>Adaptive Transfer</h2>
            <p>For students to successfully learn skills in a simulation, then apply them to a real-world situation requires an adaptive transfer of skills. “Adaptive transfer involves using one’s existing knowledge base to change a learned procedure, or to generate a solution to a completely new problem (Smith et al., 1997)”<sup>13</sup> (p. 1968). This type of learning environment helps students to adapt skills learned in a simulation to produce real-world solutions that do not directly mimic the simulated or training environment. Therefore, “adaptability can involve recognizing that a rule or strategy learned in one context can be applied to an analogous situation,”<sup>13</sup> (pg. 1967). For example, students may learn how to create a chart within the course simulation, but will then be asked to present the data in an appropriate way in the real-world requiring the skill to be used to solve a problem.</p>
            <p>Adaptive environments are more challenging for students due to problem-solving modification requirements between using previous knowledge learned and experienced during practiced events in comparison to the skills needed to solve an unknown problem unexpectedly<sup>13</sup>. Thus, research indicates that skill development “should contain elements of error investigation, as well as true acquisition of general application,”<sup>13</sup> (p. 1969). However, in a problem-solving environment that introduces an element of error, negative effects may surface. The phenomenon is known as negative transfer. Negative transfer occurs when “the source (a distractor problem) and target share surface but not structure features.”<sup>14</sup> (p. 512). </p>
            <p>This study explores negative transfer as it is or is not taking place within this introductory course. Additionally, this study is unique as it examines perceived exam concept content to that perceived simulation concept content and then explores the connection to self-efficacy for creating a spreadsheet environment to solve real-world problems such as trip/tour scheduling, or better known as the transfer appropriate processing principle<sup>15</sup>.</p>
            <h2>Experiential Learning Theory Effects upon Course Design </h2>
            <p>Kolb’s Experiential Learning Theory (ELT)<sup>1, 16</sup> provides an excellent theoretical framework to explore the course design structure and to assess student learning that will inform and guide the course design process. To actively address the iterative course redesign process, each term the instructor examines one specific aspect of the course for effectiveness of content delivery to that of skills learned (blinded for review). For the 2014/2015 academic year, the question arose regarding the conceptual knowledge transferred from the simulated environment to the real-world application, mainly the students’ adaptive transfer. </p>
            <p>To begin, the course's current structure was examined for alignment to that of Kolb's four-part experiential learning framework. “Knowledge construction has four main phases according to Kolb’s experiential learning theory (1984) including simulation, reflection, abstraction, and experimentation<sup>4</sup> (pg. 283).</p>
        </div>
    </div>

    <div class="page" id="5">
        <div class="subpage">
            <p>According to Dhulla, Kolb’s ELT “The learning process often begins with a person carrying out an action and seeing the effects of the action; the second step is to understand the effects of the action. The third step is to understand the action, and the last step is to modify the action given a new situation”<sup>17</sup> (p. 111). We then linked these steps to the components of the course under investigation, as seen in Figure 1.</p>
            <p>According to Kolb16, “immediate or Concrete Experiences are the basis for observations and Reflections. These reflections are assimilated and distilled into Abstract Concepts from which new implications for action can be drawn. These implications can be Actively Tested and serve as guides in creating new experiences” [original emphasis, p. 2]. The course under investigation follows a similar flow. Students begin with the Abstract Conceptualization by participating in a MyITLab simulation, they are then tested on the implications of what they learned in the simulation through Grader Assignments. The students complete three simulations and three grader assignments before applying what they learned to the concrete new experience of the Final Excel Exam, which is formatted as a real-world application of the skills learned. Students are then asked to reflect on the Exam experience through a course activity and, during the semesters of the study, through a short survey.</p>
            <img src="img/figure_1.png" alt="Graphic of Kolb's Learning Cycle">
            <h2>III. Methods</h2>
            <p>During the fall semester of 2014, a study was conducted in an introductory computing course for non-computer science majors. The purpose of this study is to measure the relationship between the skills learned using the simulated environment and those demonstrated on the final Excel exam. This examination also explored student confidence, comfort, and self-efficacy for applying the skills taught in the stimulation to a real-world environment. </p>
        </div>
    </div>

    <div class="page" id="6">
        <div class="subpage">
            <h3>Data Collection and Analysis</h3>
            <p>The study focused upon an introductory computer science course for non-computer science majors at a small, private institution in the southeast United States. For this study, a survey was used to collect student views on the simulation experience within the course. By completing the survey, students also gave permission for the researchers to view and access their grades on selected assignments (simulation activities and final exams) for the course. To increase our response rate, extra credit was offered to students who completed the survey, this was one of many extra credit opportunities within the course and students were rewarded the extra credit by giving their name and submitting the survey (not based on their responses). The survey was administered using Survey Monkey for students to complete outside of the course. An announcement was emailed to all students using an Oracle roster management interface due to size of the course under investigation (n=310). A reminder e-mail was sent two weeks later and the survey was left open until the end of the semester (approximately 3 weeks later). A total of 130 responses were collected for a response rate of 42%.</p>
            <p>The survey asked a number of questions about students perceived ability to use the practical software taught using the simulation such as “Do you feel confident using Microsoft Excel?”; “If you were asked to complete a project using Microsoft Excel, would you feel: comfortable?; prepared?; confident?; able to complete the project?” The survey also asked about student prior experience with Excel, how often they used MyITLab, if they attended the Exam Review lecture, and if they felt MyITLab prepared them for the exam. </p>
            <p>There were also two items on the survey that asked students to “Select all the skills you feel MyITLab prepared you for” and “Select all of the skills you felt were on the Excel exam.” Each list contained the same 19 skills that included skills a) taught using MyITLab and tested for on the Exam; b) taught using MyITLab, but not tested for on the Exam; c) not taught using MyITLab, but tested for on the Exam; and d) not taught in MyITLab nor tested for on the Exam. </p>
            <h2>IV. Results and Discussion</h2>
            <p><strong>Research Question 1: How does student interaction with a simulation transition them to working within the application when used to apply those skills in a practice/the real world setting?</strong></p>
            <p>To answer Research Question 1, we started by focusing on which skills the students felt the simulation helped prepared them to use. We counted the total number of skills marked by a student that were in fact taught in MyITLab (removing skills only on the exam or not on either) for a total of 17 skills. We then normalized that total by dividing by 17 to create a list of values between 0 and 1 creating the variable “MyITLab Skills”. The total number of skills marked as on the exam (the practical application) by the students were then counted for the skills that were on the exam (removing skills only taught in MyITLab or not on either) for a total of 13 skills. We then normalized the number of skills marked by the students by dividing by 13 creating the variable “Excel Exam Skills”. Finally, each student’s Excel Exam grade was normalized by dividing by the total number of points (50) creating the variable “Excel Exam Score”.</p>
        </div>
    </div>

    <div class="page" id="7">
        <div class="subpage">
            <p>Each of these variables were tested for correlations, seen in Table 1, using an alpha of 0.01 to indicate a significant relationship. A significant correlation with a fair relationship was found between the skills selected as those MyITLab prepared students for and the skills that students selected as on the Excel Exam. This is not surprising as many of the skills were both taught in MyITLab and tested on the Excel Exam. However, it is encouraging that students are recognizing the skills they are being taught and there application within the real-world environment.</p>
            <img src="img/tbl_1.png" alt="table 1">
            <p>A significant correlation with a fair relationship was also found between the skills identified by student as being on the exam and the score that students received on the exam. This indicated that the more skills students were able to accurately identify as on the exam, the higher their exam score was. This also is not surprising as the students who received higher grades on the exam are expected to be more familiar with the skills presented on the exam.</p>
            <p>However, using an alpha of 0.01, there was not a significant correlation and only a slight relationship between the skills students felt prepared for using MyITLab and their score on the exam. </p>
            <p>To further explore the relationship between the skills identified by the students, their exam scores, and the student’s perception of their abilities using Excel, we used an ANOVA test, again with an alpha = 0.01 to test for significant difference between the skill identification by students with varied responses to the other survey items. The significant differences are shown in Table 2. </p>
        </div>
    </div>

    <div class="page" id="8">
        <div class="subpage">
            <img src="img/tbl_2.png" alt="table 2">
            <p>Each of these significant relationships showed that students who felt more able, confident, prepared, and comfortable using Excel, on average, identified a higher number of skills taught on MyITLab and scored higher on the Exam.</p>
            <p><strong>Research Question 2: What skills can students identify and connect between the simulation and real-world application?</strong></p>
            <p>To answer Research Question 2, we looked at each of the skills individually to see how many students of the total 130 that completed the survey marked it as a skill that MyITLab prepared them for, marked it as a skill tested on the exam, and the number of students who marked the skill in both of these categories. The number of students and the percent of the overall survey participants are presented in Table 3.</p>
            <p>The most commonly marked skills as taught in MyITLab and tested on the exam were “Formatting tables” and “Formatting charts” with 85% of the total respondents marking this skill as taught in MyITLab and 83% and 84%, respectively, of the total respondents marking this skill as tested for on the Excel Exam. These also had the highest number of overlap of students who marked it on both at 72% and 71%, respectively. The formatting of tables and charts are general skill, with multiple other skills included to complete the task. These skills become a larger more memorable element within the simulations and exams.</p>
            <p>The most interesting skill on the list is the “Using IF commands” which was taught in both MyITLab and tested for on the exam, but only 55% of students marked it as taught in MyITLab with 85% marking it as tested for on the exam. This indicates that students did not feel the simulation prepared them to use this skill. This is one of the more abstract concepts, so a majority of students may have struggled to understand the application of the skill and, therefore, did not feel they learned the skill simply by using the simulation alone.</p>
            <p>The skill marked least often as taught in MyITLab was “Using IF commands” (55%). Though “Formatting the x and y axis on a chart” was taught both in MyITLab and included on the exam, this skill was marked by only 61% of the students along with “Generating random number” (taught only on the exam). The skill not taught in MyITLab or tested for on the exam (“Transposing data within an Excel spreadsheet”) was also one of the least marked skills at 62%. “Formatting the x and y axis on a chart” may have been considered part of formatting a chart and not specifically noted as an individual skill by the students resulting in the lower percentage of students marking the skill. However, overall, it was positive to see that students were accurately marking the skills taught in MyITLab.</p>

        </div>
    </div>

    <div class="page" id="9">
        <div class="subpage">
            <p>The skills that were taught in MyITLab, but not tested for on the Excel Exam comprised the lowest percentages of skills students marked as on the Exam showing that students recognized and remembered the skills needed on the exam. “Changing the alignment in documents” and “Adding a footer to a Microsoft Office file” were the lowest with 48%, followed by “Formatting the orientation of the page layout” at 56%. “Changing the Legend on a chart/table” and “Sorting data in tables” were only taught in MyITLab, but still marked by a number of students as tested for on the exam, 75% and 72% respectively. For both of these items, these skills, they may have become part of students routine for formatting tables/charts, therefore, students voluntarily completed these skills on the exam, though they were not specifically asked to show that they could complete them. “Transposing data within an Excel spreadsheet” which was not taught in MyITLab or tested on the exam yet was also marked by fewer students with only 62% marking it as on the exam</p>
            <h2>V. Conclusion and Future Work</h2>
            <p>Overall, we found that many of the students were able to identify the skills learned within the simulation and their application on a real-world problem. There was limited correlation between the identification of skills identified as taught within the simulation and those measured on the Excel exam to result in a higher grade on the test. After further exploring the data, we found that the Excel exam grade distribution did not follow a normalized bell curve, but a bimodal distribution with students receiving an A or a D on the exam. After consulting the open responses from students, we found that many students ran out of time on the exam and they may not have completed all of the tested skills they were capable of before the time ran out. Thus, data indicated a need to update the rubric for the exam to ensure points are evenly distributed based upon time and task being requested. For future semesters, the exam will be re-organized to aid students in exam navigation enabling students to show as many skills as possible at the beginning of the exam to aid them in receiving all of the points they are capable of achieving. Since ample time is provided to students in which to complete the exam as indicated in previous end of course surveys, we believe students, who are running out of time on the exam, are not able to fully demonstrate all of their skills; thus, supporting the limited correlation between the skills taught in MyITLab and the grades on the exam. Therefore, if the rubric is adjusted to place more emphasis on the spreadsheet setup and data creation, rather than the cosmetic formatting requests that are typically completed at the end of  the project, a normalized bell curve will then be achieved. Furthermore, additional simulation activities that include a greater focus upon the areas in which students believed were missing between the two activities will be explored for curriculum alignment. Overall, the implementation of simulated activities within the course was found to reflectively engage students with the content of the activity and provide students with a true experimental environment in order to create a real-world project. </p>
        </div>
    </div>






    <div class="page" id="10">
        <div class="subpage">
            <h2>References</h2>
            <ol>
                <li>Kolb, D. A. (1984) Experiential Learning: Experience as the source of learning and development. </li>
                <li>Scachitti, S.; Salina, J. and Karanam, D. (2009) Minding the Big Picture: Using discrete event process simulation as a problem solving tool for students. </li>
                <li>Lattuca, L. R.; Terenzini, P. T. and Volkwein, J. F. (2006) Engineering Change: A Study of the Impact of EC2000. </li>
                <li>Abdulwahed, M. & Nagy, Z. K. (2009) Applying Kolb's Experiential Learning Cycle for Laboratory Education. Journal of Engineering Education, 98(3) 283-294.</li>
                <li>Davidovitch, L.; Parush, A. and Shtub, A. (2006) Simulation-based learning in engineering education: Performance and transfer in learning project management. Journal of Engineering Education, 95(4) 289-299.</li>
                <li>Koretsky, M.; Kelly, C. and Gummer, E. (2011) Student Perceptions of Learning in the Laboratory: Comparison of industrially situated virtual laboratories to capstone physical laboratories. Journal of Engineering Education, 100(3) 540-573.</li>
                <li>Steadman, R. H.; Coates, W. C.; Huang, Y. M., et al (2006) Simulation-based training is superior to problem-based learning for acquisition of critical assessment and management skills. Critical Care Medicine, 34(1) 151-157.</li>
                <li>Curtin, L. B.; Finn, L. A.; Czosnowski, Q. A.; Whiman, C. B. and Cawley, M. J. (2011) Computer-based simulation training to improve learning outcomes in mannequin-based simulation exercises. American Journal of Pharmaceutical Education, 75(6) 1-6.</li>
                <li>De Jong, T. & Van Joolingen, W. (1998) Scientific discovery learning with computer simulations of conceptual domains. Review of Educational Research, 68179-202.</li>
                <li>Kolloffel, B. & De Jong, T. (2013) Conceptual understanding of electrical circuits in secondary vocational engineering education: Combining traditional instruction with inquiry learning in a virtual lab. Journal of Engineering Education, 102(3) 375-393.</li>
                <li>Morgan, P. J.; Cleave-Hogg, D.; Desousa, S. and Lam-McCullough, J. (2006) Applying theory to practice in undergraduate education using high fidelity simulation. Medical Teacher, 28(1) e10-e15.</li>
                <li>Rutten, N.; Van Joolingen, W. and van der Veen, J. (2012) The learning effects of computer simulations in science education. Computers & Education, 58136-153.</li>
                <li>Ivancic, K. & Hesketh, B. (2000) Learning from errors in a driving simulation: effects on driving skill and self-confidence. Ergonomics, 43(12) 1966-1984.</li>
                <li>Novick, L. R. (1988) Analogical transfer, problem similarity, and expertise. Journal of Experimental Psychology: Learning, memory, and cognition, 14(3) 510-520.</li>
                <li>Morris, C. D.; Bransford, J. D. and Franks, J. J. (1997) Levels of processing versus transfer appropriate processing. Journal of Verbal Learning and Verbal Behavior, 16519-533.</li>
                <li>Kolb, D. A.; Boyatzis, R. E. and Mainemelis, C. (2001) Experiential Learning Theory: Previous research and new directions. Pers, 227.</li>
                <li>Dhulla, T. V. (2014) Management game designed for experiential learning: A teaching pedagogy. Journal of International Academic Research for Multidisciplinary, 2(5) 110-113.</li>
            </ol>
        </div>
    </div>
</div>
</body>
</html>